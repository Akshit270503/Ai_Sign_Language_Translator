# ğŸ¤Ÿ SignSense AI â€” Real-Time Sign Language Translator
### AI-Powered ASL Detection using MediaPipe + Flask

---

## ğŸ¯ What This Does
- Detects **hand gestures** from your webcam in real time
- Classifies **ASL letters (Aâ€“Y)** and **common signs** (Hello, I Love You, OK, Peace, etc.)
- Builds **sentences** from detected signs
- **Speaks** the sentence aloud using browser TTS
- Beautiful **sci-fi dark UI** with live confidence bars and landmark visualization

---

## ğŸ› ï¸ Setup (5 Minutes)

### 1. Install Python (3.9â€“3.11 recommended)
Download from https://python.org

### 2. Install Dependencies
```bash
pip install flask opencv-python mediapipe numpy
```

### 3. Run the App
```bash
python app.py
```

### 4. Open Browser
```
http://127.0.0.1:5000
```

**Allow webcam access** when prompted. That's it! ğŸ‰

---

## ğŸ“ Project Structure
```
sign_language_translator/
â”‚
â”œâ”€â”€ app.py              â† Main Flask app (run this)
â”œâ”€â”€ collect_data.py     â† Collect your own gesture data
â”œâ”€â”€ train_model.py      â† Train ML model on collected data
â”œâ”€â”€ requirements.txt    â† Python dependencies
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html      â† Beautiful web UI
â”‚
â”œâ”€â”€ dataset/            â† Your collected gesture data (CSV)
â””â”€â”€ model/              â† Trained model files
```

---

## ğŸ¤™ Supported Signs

### Letters
A, B, C, D, E, F, G, H, I, K, L, M, N, O, P, R, S, T, U, V, W, X, Y

### Word Signs
| Sign | Hand Gesture |
|---|---|
| HELLO ğŸ‘‹ | All 5 fingers open |
| I LOVE YOU â¤ï¸ | Thumb + Index + Pinky up |
| OK ğŸ‘Œ | Thumb & Index circle |
| PEACE âœŒï¸ | Index + Middle (spread) |
| ROCK ON ğŸ¤˜ | Index + Pinky up |
| STOP âœ‹ | Open palm facing out |
| POINTING â˜ï¸ | Only index finger up |

---

## ğŸ§  Train Your Own Model (Advanced)

The default app uses a rule-based classifier. For higher accuracy with more signs:

### Step 1 â€” Collect Data
```bash
python collect_data.py
```
- Press a **letter key** (Aâ€“Z) to start recording that sign
- Perform the gesture for ~10 seconds (â‰ˆ300 samples)
- Press **Space** to stop, then record the next sign
- Press **S** to save, **Q** to quit

### Step 2 â€” Train Model
```bash
# Install extra deps
pip install scikit-learn tensorflow pandas

python train_model.py
```
The trained model auto-saves to `model/` and the app will use it automatically.

---

## âš™ï¸ Tech Stack

| Component | Technology |
|---|---|
| Backend | Python + Flask |
| Hand Tracking | Google MediaPipe (21 landmarks) |
| Computer Vision | OpenCV |
| Classification | Rule-based + Optional Neural Network |
| Frontend | HTML5 + CSS3 + Vanilla JS |
| Text-to-Speech | Web Speech API (browser built-in) |
| Real-time Stream | MJPEG over HTTP |

---

## ğŸ”® Future Enhancements (for your report)
- LSTM model for dynamic (word-level) signs
- Indian Sign Language (ISL) support
- Avatar animation for reverse translation (text â†’ signs)
- Multi-language TTS output
- Mobile app version using React Native
- Cloud deployment on Heroku/Render

---

## ğŸ‘¨â€ğŸ’» Built With
- Python 3.x
- MediaPipe by Google
- OpenCV
- Flask

---

*This project demonstrates real-time computer vision, hand landmark detection, gesture classification, and web streaming â€” all in a single clean Python application.*
